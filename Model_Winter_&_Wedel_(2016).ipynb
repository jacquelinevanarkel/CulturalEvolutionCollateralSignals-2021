{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Winter & Wedel (2016).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMUQ0PvzPAQNhLpZhP+xTaD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacquelinevanarkel/CulturalEvolutionCollateralSignals-2021/blob/main/Model_Winter_%26_Wedel_(2016).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_zZBw6QKYo4"
      },
      "source": [
        "# Replication of Winter & Wedel (2016) / Wedel (2012)\n",
        "\n",
        "Because we decided to first replicate the study, everything about the collateral signals is left out (but taken into account for the ease of implementation later on). After the resutls are replicated, we adjust the model to introduce a division between word categories: communicative words and metacommunicative words.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_56j5zf_K7YK"
      },
      "source": [
        "## General features\n",
        "\n",
        "- An **agent** consists of a lexicon of $|W|$ word categories (let's start with 4). ~~One word in that set should be singled out as the *continuer* word. In other words, the entire set of word categories $W$ can be further split up into a set of regular \"communicative\" words $C$ and a set of \"metacommunicative\" words $M$, where $W = C \\cup M$ and $C \\cap M = \\varnothing$. Let's start with $|M| = 1$. For the word(s) in the set $M$, additional and/or adapted pressures will apply.~~\n",
        "    - A **word** is represented by a set of exemplars. \n",
        "        - An **exemplar** is represented by a vector that designates a point in an *n*-dimensional space. Let's start with 2 dimensions, and let's have them both range arbitrarily from 0 to 100 (with integer values in between). An example of an exemplar would then be $[15, 25]$ (these individual values in the vector, like 15 and 25, are referred to as \"segments\" by Wedel, 2012).\n",
        "\n",
        "\n",
        "- **Initialisation of an agent:** <span class=\"mark\">Just a randomly generated seed set of exemplars for each word category?</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzpNaOhse6Cv"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CEl3fbmLAIx"
      },
      "source": [
        "class Agent():\n",
        "\n",
        "  def __init__(self, n_words, n_dimensions, n_exemplars = 100, n_continuers=0, similarity_bias_word=True, similarity_bias_segment=True, noise=True, anti_ambiguity_bias=True):\n",
        "    \n",
        "    self.n_words = n_words\n",
        "    self.n_dimensions = n_dimensions\n",
        "    self.n_exemplars = n_exemplars\n",
        "    self.n_continuers = n_continuers\n",
        "    self.similarity_bias_word = similarity_bias_word\n",
        "    self.similarity_bias_segment = similarity_bias_segment\n",
        "    self.noise = noise\n",
        "    self.anti_ambiguity_bias = anti_ambiguity_bias\n",
        "\n",
        "    # Generate a lexicon as part of the initialisation\n",
        "    self.lexicon, self.com_words, self.meta_com_words = self.generate_lexicon()\n",
        "\n",
        "  # Initialising lexicon\n",
        "  def generate_lexicon(self):\n",
        "\n",
        "    # Create a lexicon consisting of n_words words each in turn consisting of n_exemplars exemplars\n",
        "    lexicon = []\n",
        "    for w in range(self.n_words):\n",
        "      word = []\n",
        "\n",
        "      # Define the mean and the covariance to sample from a multivariate normal distribution to create clustered exemplars for the words\n",
        "      mean = [random.randrange(10, 91) for i in range(self.n_dimensions)]\n",
        "      cov = [[10, 0], [0, 10]]\n",
        "      x, y = np.random.multivariate_normal(mean, cov, self.n_exemplars).T\n",
        "      word.append(list(map(lambda x, y: [x, y], x, y)))\n",
        "      \n",
        "      # Plot every word\n",
        "      plt.scatter(x, y)\n",
        "\n",
        "      # Initialiase all words as 'communicative words' ('C')\n",
        "      lexicon.append([word[0], \"C\"])\n",
        "\n",
        "    # print(lexicon)\n",
        "\n",
        "    # Some plot settings\n",
        "    plt.xlim(0, 100)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.show()\n",
        "\n",
        "    # Split the lexicon into meta communicative words (continuers) and communicative words\n",
        "    if self.n_continuers:\n",
        "      if self.n_continuers > self.n_words:\n",
        "        raise ValueError(\"The number of continuers must be lower than the number of words.\")\n",
        "\n",
        "      # The continuers are randomly chosen out of the lexicon\n",
        "      indices_meta = random.sample(range(self.n_words), k=self.n_continuers)\n",
        "      meta_com_words = []\n",
        "      for index in indices_meta:\n",
        "        lexicon[index][1] = \"M\"\n",
        "        # Create a separate lexicon with the meta communicative words\n",
        "        meta_com_words.append(lexicon[index])\n",
        "\n",
        "      # The words that are not meta communicative words are communicative words\n",
        "      com_words = [word for word in lexicon if word not in meta_com_words]\n",
        "\n",
        "      # print(\"The word categories are split into communicative and metacommunicative words\")\n",
        "      # print(\"New lexicon:\", lexicon)\n",
        "\n",
        "      # print(\"Meta:\", meta_com_words)\n",
        "      # print(\"Com:\", com_words)\n",
        "\n",
        "    # If there are no continuers, the meta communicative words list is empty and all the words in the lexicon are communicative words\n",
        "    else:\n",
        "      com_words = lexicon\n",
        "      meta_com_words = []\n",
        "\n",
        "    return lexicon, com_words, meta_com_words"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "jQhNSnuBd3-6",
        "outputId": "c1a93c0e-29f8-45f1-caf9-ee1daf7e3253"
      },
      "source": [
        "# Test the initialisation\n",
        "agent_test = Agent(4, 2)"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VEVHMBGKkhZ"
      },
      "source": [
        "## Production:\n",
        "\n",
        "Production begins by selecting one exemplar from a word category: An exemplar is chosen from the target word category with probability proportional to the exemplar's activation level. \n",
        "- **Activation level:** \n",
        "    - _\"Each new exemplar is associated with an initial activation value that decreases over time (i.e., memory decay; Hintzman, 1986; Nosofsky, 1986; Pierrehumbert, 2001).\"_ (Winter & Wedel, 2016)\n",
        "    - _\"Activation is calculated as an exponential function of recency, where exemplars that were stored 100 rounds previously have an activation level that is approximately .1% that of a new exemplar).\"_ (Wedel, 2012)\n",
        "    - _\"In the model runs shown here, the activation of a exemplar is modeled as $e^{0.2j}$, where $j$ is its list position; this results in a exemplar at position 100 having an activation that is approximately .01 times that of an exemplar at position 1.\"_ (Wedel, 2012; Appendix)\n",
        "    - _\"The probability of an exemplar being chosen as a production target is its activation relative to the total activation of all exemplars in the category.\"_ (Wedel, 2012; Appendix)\n",
        "    - _\"Exemplars at list positions greater than 100 are discarded after every round to keep computation efficient; preserving more exemplars slows the rate of change in the system but otherwise does not qualitatively change system behavior.\"_ (Wedel, 2012; Appendix)\n",
        "  \n",
        "Before this target is passed to the listener however, two biases are applied to it:\"\n",
        "- **Similarity biases:** Consists of the following two components:\n",
        "    - **Within-word category similarity bias:** _\"The segment exemplar values of this initial word target are stochastically biased toward the value at the same positions in all the word exemplars within the category\"_\n",
        "        - _\"At the word level, population vectors are calculated for the segment values in the target word relative to all segment values at the same position over all exemplars within that word category\"_ (Wedel, 2012; Appendix)\n",
        "    - **Within-segment-dimension similarity bias:** _\"each individual segment exemplar value in the target is also stochastically biased toward all other segment exemplars that reference the same dimension across the entire lexicon\"_ \n",
        "        - _\"At the segment level, population vectors are calculated for the segment values in the target word relative to all segment values on that dimension across the lexicon.\"_ (Wedel, 2012; Appendix)\n",
        "    - _\"To model the influence of both word and segment recency and similarity on production variation, the population vectors at each segment dimension at each level are combined to create a new output that combines information from both within-word category, and within-lexicon sources. The relative contribution of word versus segment population vectors to the output was fixed at .9.\"_ (Wedel, 2012; Appendix) <span class=\"mark\"> --> Wait, what does that last bit mean exactly? That the segment-level population vector contributes 9/10th of what the word-level population vector contributes? Or the other way around? Or something else entirely? Not sure how to interpret this... The Wedel (2012) Appendix refers to the following two articles in this context: (Guenther and Gjaja 1996; Oudeyer 2002); maybe those references could help us? (Also quite likely they won't though.) </span>\n",
        "    - _\"The population vector with respect to a particular point within a particular segment dimension is a weighted average of all segment exemplars mapped to the category, where both the Euclidean distance from the target exemplar and activation influence each exemplar’s contribution. This is conceptually the same as Nosofsky’s Generalized Context model (Nosofsky 1988), modified to take exemplar activation into account. The formula used to incorporate these factors is given below, where $p$ is the output population vector, y is each position within the segment dimension value of the target under production, $w_y$ is the activation of the exemplar, $x$ is the reference point chosen as the basis for production, and $k$ is a scaling factor influencing the fall off of the contribution to the population vector of the point $y$ relative to $x$:\"_ (Wedel, 2012; Appendix)\n",
        "\n",
        "$$ p = \\frac{\\sum_y yw_{y} e^{-k |x-y|}}{\\sum_{y} w_{y} e^{-k |x-y|}} $$\n",
        "\n",
        "- _\"The value of $k$ used in the simulations shown here is 0.2; a larger value of $k$ reduces the effect of more distant values on the population vector.\"_ (Wedel, 2012; Appendix). --> Let us start with $k = 0.2$ as well, but make $k$ into a parameter that we can change just in case.\n",
        "\n",
        "\n",
        "- **Random noise:** _\"Noise is added to values of the output target by adding a normally distributed random value. This random value is biased slightly toward the center of the dimension, (i.e. a scale value of 50), in a simple model of production-based lenition (Pierrehumbert 2001; see also e.g. Lindblom et al. 1984 for arguments that the packing of phoneme inventories is in part a consequence of effort-minimization processes). The results described below do not depend on this lenition bias, but they contribute to the illustration by imposing a tendency for each segment exemplar distribution to drift toward the center of each dimension which encourages category merger (see discussion below).\"_ \n",
        "    - _\"Finally, a Gaussian random variable with a standard deviation of 3 is added to the output to introduce noise. This variable is biased slightly toward the center of the dimensional space, creating a fixed attractor at the center of each segment dimension in the system. The bias is calculated using a parabolic response curve given below, where $b$ is the bias added to the output population vector, $p$ is the output population vector, $N$ is the number of points in the space and $G$ is a constant; $b$ is subtracted from outputs greater than $N/2$ (here, 50) and added to those below it.\"_ (Wedel, 2012; Appendix)\n",
        "    \n",
        "$$ b = \\frac{(p-N/2)^2}{G}$$\n",
        "\n",
        "- _\"The value of G used in these simulations was 5000, giving a bias toward the center of 0.5 at the edges of the continuum. All else being equal, this bias shifts the distributions of both categories toward the center of the dimension over time, i.e. toward 50, which corresponds to a simple model of articulatory undershoot (cf. Lindblom 1983; Pierrehumbert 2001).\"_ (Wedel, 2012; Appendix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjHSAE9bS6yp"
      },
      "source": [
        "class Production(Agent):\n",
        "\n",
        "  def __init__(self, similarity_bias_word, similarity_bias_segment, noise):\n",
        "        super().__init__(similarity_bias_word, similarity_bias_segment, noise, lexicon)\n",
        "\n",
        "  def select_exemplar(self, word_index):\n",
        "\n",
        "    # Only store exemplars until position 100\n",
        "    exemplars = lexicon[word_index][1][:101]\n",
        "\n",
        "    activation_exemplars = []\n",
        "    j = 1\n",
        "    for exemplar in exemplars: \n",
        "      activation = math.exp(0.2*j)\n",
        "      activation_exemplars.append(activation)\n",
        "      j += 1\n",
        "\n",
        "    activation_word = sum(activation_exemplars)\n",
        "\n",
        "    exemplar_prob = activation/activation_word\n",
        "\n",
        "    return self.add_biases(exemplar)\n",
        "\n",
        "  def add_biases(self):\n",
        "    \n",
        "    if self.similarity_bias_word:\n",
        "      exemplar = self.similarity_bias_word()\n",
        "\n",
        "    if self.similarity_bias_segment:\n",
        "      exemplar = self.similarity_bias_segment()\n",
        "\n",
        "    if self.noise:\n",
        "      exemplar = self.noise()\n",
        "\n",
        "    return exemplar\n",
        "\n",
        "  def similarity_bias_word(self):\n",
        "\n",
        "    return exemplar\n",
        "\n",
        "  def similarity_bias_segment(self):\n",
        "\n",
        "    return exemplar\n",
        "\n",
        "  def noise(self):\n",
        "\n",
        "    return exemplar"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0cSEPbDr4h8",
        "outputId": "2d009d67-c4c9-4736-cfb4-a43650766700",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = math.exp(20)\n",
        "y = math.exp(0.2)\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "print(x/y)"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "485165195.4097903\n",
            "1.2214027581601699\n",
            "397219665.8050838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgzt8ZFPKq1K"
      },
      "source": [
        "##Perception:\n",
        "        \n",
        "_\"begins the categorization process by calculating the similarity of the speaker output to each category’s stored word exemplars given their activations, in a variant of the Generalized Context Model (Nosofsky 1988). The overall similarities of the speaker output to each category are interpreted as a relative goodness of fit, and the speaker output is then stored as a new exemplar in the best fitting category.\"_\n",
        "\n",
        "- **Anti-ambiguity bias:** From Winter & Wedel (2016): _\"A final feature of the model is a bias against confusability of word perception, that is, an anti-ambiguity bias. The bias is implemented as follows: the probability of successful identification of an output with a word category is proportional to the degree to which the output uniquely maps to that category and to no other. In this way, distinctive speaker outputs are more likely to be stored than ambiguous outputs, with the result that distinctive phonetic values contribute more to the continuing evolution of the lexicon, both at the word and sound levels.\"_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LIhWa6ZYpEc"
      },
      "source": [
        "class Perception(Agent):\n",
        "\n",
        "  def __init__(self, anti_ambiguity_bias, signal):\n",
        "      super().__init__(anti_ambiguity_bias)\n",
        "      self.signal = signal\n",
        "\n",
        "  def similarity(self):\n",
        "\n",
        "    return similarities\n",
        "\n",
        "  def anti_ambiguity_bias(self, similarities):\n",
        "\n",
        "    return probability_storage\n",
        "\n",
        "  def store_signal(self, probability_storage):\n",
        "\n",
        "    return lexicon"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}