{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Winter & Wedel (2016).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOsxL3BB/HfiXTypOAg4XWX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacquelinevanarkel/CulturalEvolutionCollateralSignals-2021/blob/main/Model_Winter_%26_Wedel_(2016).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_zZBw6QKYo4"
      },
      "source": [
        "# Replication of Winter & Wedel (2016) / Wedel (2012)\n",
        "\n",
        "Because we decided to first replicate the study, everything about the collateral signals is left out (but taken into account for the ease of implementation later on). After the resutls are replicated, we adjust the model to introduce a division between word categories: communicative words and metacommunicative words.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_56j5zf_K7YK"
      },
      "source": [
        "## General features\n",
        "\n",
        "- An **agent** consists of a lexicon of $|W|$ word categories (let's start with 4). ~~One word in that set should be singled out as the *continuer* word. In other words, the entire set of word categories $W$ can be further split up into a set of regular \"communicative\" words $C$ and a set of \"metacommunicative\" words $M$, where $W = C \\cup M$ and $C \\cap M = \\varnothing$. Let's start with $|M| = 1$. For the word(s) in the set $M$, additional and/or adapted pressures will apply.~~\n",
        "    - A **word** is represented by a set of exemplars. \n",
        "        - An **exemplar** is represented by a vector that designates a point in an *n*-dimensional space. Let's start with 2 dimensions, and let's have them both range arbitrarily from 0 to 100 (with integer values in between). An example of an exemplar would then be $[15, 25]$ (these individual values in the vector, like 15 and 25, are referred to as \"segments\" by Wedel, 2012).\n",
        "\n",
        "\n",
        "- **Initialisation of an agent:** <span class=\"mark\">Just a randomly generated seed set of exemplars for each word category?</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzpNaOhse6Cv"
      },
      "source": [
        "import random"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CEl3fbmLAIx"
      },
      "source": [
        "class Agent():\n",
        "\n",
        "  def __init__(self, n_words, n_exemplars, n_dimensions, n_continuers=0, similarity_bias_word=True, similarity_bias_segment=True, noise=True, anti_ambiguity_bias=True):\n",
        "    \n",
        "    self.n_words = n_words\n",
        "    self.n_exemplars = n_exemplars\n",
        "    self.n_dimensions = n_dimensions\n",
        "    self.n_continuers = n_continuers\n",
        "    self.similarity_bias_word = similarity_bias_word\n",
        "    self.similarity_bias_segment = similarity_bias_segment\n",
        "    self.noise = noise\n",
        "    self.anti_ambiguity_bias = anti_ambiguity_bias\n",
        "\n",
        "    # Generate a lexicon as part of the initialisation\n",
        "    self.lexicon, self.com_words, self.meta_com_words = self.generate_lexicon()\n",
        "\n",
        "  # Initialising lexicon\n",
        "  def generate_lexicon(self):\n",
        "\n",
        "    # Create a lexicon consisting of n_words words each in turn consisting of n_exemplars exemplars\n",
        "    lexicon = []\n",
        "    for w in range(self.n_words):\n",
        "      word = []\n",
        "      # Initialise all words with n_exemplars exemplars\n",
        "      for exem in range(self.n_exemplars):\n",
        "        # The exemplars consist of n_dimensions, which are randomly chosen from values between 0-100\n",
        "        exemplar = [random.randrange(101) for i in range(self.n_dimensions)]\n",
        "        word.append(exemplar)\n",
        "      # Initialiase all words as 'communicative words' ('C')\n",
        "      lexicon.append([word, \"C\"])\n",
        "\n",
        "    # print(lexicon)\n",
        "\n",
        "    # Split the lexicon into meta communicative words (continuers) and communicative words\n",
        "    if self.n_continuers:\n",
        "      if self.n_continuers > self.n_words:\n",
        "        raise ValueError(\"The number of continuers must be lower than the number of words.\")\n",
        "\n",
        "      # The continuers are randomly chosen out of the lexicon\n",
        "      indices_meta = random.sample(range(self.n_words), k=self.n_continuers)\n",
        "      meta_com_words = []\n",
        "      for index in indices_meta:\n",
        "        lexicon[index][1] = \"M\"\n",
        "        # Create a separate lexicon with the meta communicative words\n",
        "        meta_com_words.append(lexicon[index])\n",
        "\n",
        "      # The words that are not meta communicative words are communicative words\n",
        "      com_words = [word for word in lexicon if word not in meta_com_words]\n",
        "\n",
        "      # print(\"The word categories are split into communicative and metacommunicative words\")\n",
        "      # print(\"New lexicon:\", lexicon)\n",
        "\n",
        "      # print(\"Meta:\", meta_com_words)\n",
        "      # print(\"Com:\", com_words)\n",
        "\n",
        "    # If there are no continuers, the meta communicative words list is empty and all the words in the lexicon are communicative words\n",
        "    else:\n",
        "      com_words = lexicon\n",
        "      meta_com_words = []\n",
        "\n",
        "    return lexicon, com_words, meta_com_words"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQhNSnuBd3-6"
      },
      "source": [
        "# Test the initialisation\n",
        "agent_test = Agent(3, 4, 2, 1)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VEVHMBGKkhZ"
      },
      "source": [
        "##Production:\n",
        "\n",
        "Production begins by selecting one exemplar from a word category: An exemplar is chosen from the target word category with probability proportional to the exemplar's activation level. \n",
        "- **Activation level:** _\"Activation is calculated as an exponential function of recency, where exemplars that were stored 100 rounds previously have an activation level that is approximately .1% that of a new exemplar).\"_\n",
        "  \n",
        "Before this target is passed to the listener however, two biases are applied to it:\"\n",
        "- **Similarity bias:** Consists of the following two components:\n",
        "    - **Within-word category similarity bias:** _\"The segment exemplar values of this initial word target are stochastically biased toward the value at the same positions in all the word exemplars within the category\"_\n",
        "    - **Within-segment-dimension similarity bias:** _\"each individual segment exemplar value in the target is also stochastically biased toward all other segment exemplars that reference the same dimension across the entire lexicon\"_ \n",
        "- **Random noise:** _\"Noise is added to values of the output target by adding a normally distributed random value. This random value is biased slightly toward the center of the dimension, (i.e. a scale value of 50), in a simple model of production-based lenition (Pierrehumbert 2001; see also e.g. Lindblom et al. 1984 for arguments that the packing of phoneme inventories is in part a consequence of effort-minimization processes). The results described below do not depend on this lenition bias, but they contribute to the illustration by imposing a tendency for each segment exemplar distribution to drift toward the center of each dimension which encourages category merger (see discussion below).\"_ \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjHSAE9bS6yp"
      },
      "source": [
        "class Production(Agent):\n",
        "\n",
        "  def __init__(self, similarity_bias_word, similarity_bias_segment, noise):\n",
        "        super().__init__(similarity_bias_word, similarity_bias_segment, noise)\n",
        "\n",
        "  def select_exemplar(self):\n",
        "\n",
        "    return self.add_biases(exemplar)\n",
        "\n",
        "  def add_biases(self):\n",
        "    \n",
        "    if self.similarity_bias_word:\n",
        "      exemplar = self.similarity_bias_word()\n",
        "\n",
        "    if self.similarity_bias_segment:\n",
        "      exemplar = self.similarity_bias_segment()\n",
        "\n",
        "    if self.noise:\n",
        "      exemplar = self.noise()\n",
        "\n",
        "    return exemplar\n",
        "\n",
        "  def similarity_bias_word(self):\n",
        "\n",
        "    return exemplar\n",
        "\n",
        "  def similarity_bias_segment(self):\n",
        "\n",
        "    return exemplar\n",
        "\n",
        "  def noise(self):\n",
        "\n",
        "    return exemplar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgzt8ZFPKq1K"
      },
      "source": [
        "##Perception:\n",
        "        \n",
        "_\"begins the categorization process by calculating the similarity of the speaker output to each category’s stored word exemplars given their activations, in a variant of the Generalized Context Model (Nosofsky 1988). The overall similarities of the speaker output to each category are interpreted as a relative goodness of fit, and the speaker output is then stored as a new exemplar in the best fitting category.\"_\n",
        "\n",
        "- **Anti-ambiguity bias:** From Winter & Wedel (2016): _\"A final feature of the model is a bias against confusability of word perception, that is, an anti-ambiguity bias. The bias is implemented as follows: the probability of successful identification of an output with a word category is proportional to the degree to which the output uniquely maps to that category and to no other. In this way, distinctive speaker outputs are more likely to be stored than ambiguous outputs, with the result that distinctive phonetic values contribute more to the continuing evolution of the lexicon, both at the word and sound levels.\"_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LIhWa6ZYpEc"
      },
      "source": [
        "class Perception(Agent):\n",
        "\n",
        "  def __init__(self, anti_ambiguity_bias, signal):\n",
        "      super().__init__(anti_ambiguity_bias)\n",
        "      self.signal = signal\n",
        "\n",
        "  def similarity(self):\n",
        "\n",
        "    return similarities\n",
        "\n",
        "  def anti_ambiguity_bias(self, similarities):\n",
        "\n",
        "    return probability_storage\n",
        "\n",
        "  def store_signal(self, probability_storage):\n",
        "\n",
        "    return lexicon"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}