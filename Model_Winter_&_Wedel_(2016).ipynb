{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Winter & Wedel (2016).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFRnhpRUU7UKVX05sFYfX8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacquelinevanarkel/CulturalEvolutionCollateralSignals-2021/blob/main/Model_Winter_%26_Wedel_(2016).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_zZBw6QKYo4"
      },
      "source": [
        "# Replication of Winter & Wedel (2016) / Wedel (2012)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_56j5zf_K7YK"
      },
      "source": [
        "## General features\n",
        "\n",
        "- An **agent** consists of a lexicon of $|W|$ word categories (let's start with 4). One word in that set should be singled out as the *continuer* word. In other words, the entire set of word categories $W$ can be further split up into a set of regular \"communicative\" words $C$ and a set of \"metacommunicative\" words $M$, where $W = C \\cup M$ and $C \\cap M = \\varnothing$. Let's start with $|M| = 1$. For the word(s) in the set $M$, additional and/or adapted pressures will apply.\n",
        "    - A **word** is represented by a set of exemplars. \n",
        "        - An **exemplar** is represented by a vector that designates a point in an *n*-dimensional space. Let's start with 2 dimensions, and let's have them both range arbitrarily from 0 to 100 (with integer values in between). An example of an exemplar would then be $[15, 25]$ (these individual values in the vector, like 15 and 25, are referred to as \"segments\" by Wedel, 2012).\n",
        "\n",
        "\n",
        "- **Initialisation of an agent:** <span class=\"mark\">Just a randomly generated seed set of exemplars for each word category?</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CEl3fbmLAIx"
      },
      "source": [
        "# Initialising lexicon\n",
        "\n",
        "# Initialising agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VEVHMBGKkhZ"
      },
      "source": [
        "##Production:\n",
        "\n",
        "Production begins by selecting one exemplar from a word category: An exemplar is chosen from the target word category with probability proportional to the exemplar's activation level. \n",
        "- **Activation level:** _\"Activation is calculated as an exponential function of recency, where exemplars that were stored 100 rounds previously have an activation level that is approximately .1% that of a new exemplar).\"_\n",
        "  \n",
        "Before this target is passed to the listener however, two biases are applied to it:\"\n",
        "- **Similarity bias:** Consists of the following two components:\n",
        "    - **Within-word category similarity bias:** _\"The segment exemplar values of this initial word target are stochastically biased toward the value at the same positions in all the word exemplars within the category\"_\n",
        "    - **Within-segment-dimension similarity bias:** _\"each individual segment exemplar value in the target is also stochastically biased toward all other segment exemplars that reference the same dimension across the entire lexicon\"_ \n",
        "- **Random noise:** _\"Noise is added to values of the output target by adding a normally distributed random value. This random value is biased slightly toward the center of the dimension, (i.e. a scale value of 50), in a simple model of production-based lenition (Pierrehumbert 2001; see also e.g. Lindblom et al. 1984 for arguments that the packing of phoneme inventories is in part a consequence of effort-minimization processes). The results described below do not depend on this lenition bias, but they contribute to the illustration by imposing a tendency for each segment exemplar distribution to drift toward the center of each dimension which encourages category merger (see discussion below).\"_ \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgzt8ZFPKq1K"
      },
      "source": [
        "##Perception:\n",
        "        \n",
        "_\"begins the categorization process by calculating the similarity of the speaker output to each categoryâ€™s stored word exemplars given their activations, in a variant of the Generalized Context Model (Nosofsky 1988). The overall similarities of the speaker output to each category are interpreted as a relative goodness of fit, and the speaker output is then stored as a new exemplar in the best fitting category.\"_\n",
        "\n",
        "- **Anti-ambiguity bias:** From Winter & Wedel (2016): _\"A final feature of the model is a bias against confusability of word perception, that is, an anti-ambiguity bias. The bias is implemented as follows: the probability of successful identification of an output with a word category is proportional to the degree to which the output uniquely maps to that category and to no other. In this way, distinctive speaker outputs are more likely to be stored than ambiguous outputs, with the result that distinctive phonetic values contribute more to the continuing evolution of the lexicon, both at the word and sound levels.\"_"
      ]
    }
  ]
}